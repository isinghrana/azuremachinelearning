{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Diabetes Classification Model Training using AutoML\n",
    "Sample Notebook to demonstrate creation of a Machine Learning Model to predict likelihood of diabetes using Azure Machine Learning Services. \n",
    "\n",
    "* Azure AutoML was used to automatically select the algorithm and hyper-parameters.\n",
    "* Another important point demonstrated in the sample is decorating the Scoring code with attributes so that Inferencing Web Service can be easily consumed by Power BI\n",
    "\n",
    "This is a two-part solution, this first notebook is used to train the model and then create a Docker Image to be used for inferencing and the second notebook <a href=\"./deploy_model.ipynb\">deploy_model</a> shows how to deploy the trained model to Azure Kubernetes Cluster.\n",
    "\n",
    "** Please note that this is just a sample to demonstrate the capability of the service but does not gaurantee the quality beyond this demo scope**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from azureml.core.compute import AksCompute, AmlCompute, ComputeTarget\n",
    "from azureml.core import Datastore\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "Initialize connection to AML Workspace and set variables to be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Update the config settings as per your environment\n",
    "subscription_id = \"<TODO>\"\n",
    "resource_group = \"<TODO>\"\n",
    "workspace_name = \"<TODO>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n",
    "    # write the details of the workspace to a configuration file to the notebook library\n",
    "    ws.write_config()\n",
    "    print(\"Workspace configuration succeeded. Skip the workspace creation steps below\")\n",
    "except:\n",
    "    print(\"Workspace not accessible. Change your parameters or create a new workspace below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load workspace configuration from the config.json file in the current folder.\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.location, ws.resource_group, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for your training cluster.\n",
    "amlcompute_cluster_name = \"traincluster\"\n",
    "experiment_name = 'diabetes-classification'\n",
    "project_folder = './project-temp-files'\n",
    "\n",
    "image_name = \"diabclassprob\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Target for Training\n",
    "Training is peformed on a remote AML Compute cluster. The AML Workspace is queried for a list of existing Compute Targets, an existing cluster is used if one exists (determined based on the Compute Target and Type) otherwise a new cluster is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "found = False\n",
    "\n",
    "# Check if this compute target already exists in the workspace.\n",
    "\n",
    "cts = ws.compute_targets\n",
    "if amlcompute_cluster_name in cts and cts[amlcompute_cluster_name].type == 'AmlCompute':\n",
    "    found = True\n",
    "    print('Found existing compute target.')\n",
    "    compute_target = cts[amlcompute_cluster_name]\n",
    "\n",
    "if not found:\n",
    "    print('Creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = \"STANDARD_D2_V2\", # for GPU, use \"STANDARD_NC6\"\n",
    "                                                                #vm_priority = 'lowpriority', # optional\n",
    "                                                                max_nodes = 2)\n",
    "\n",
    "    # Create the cluster.\\n\",\n",
    "    compute_target = ComputeTarget.create(ws, amlcompute_cluster_name, provisioning_config)\n",
    "\n",
    "    # Can poll for a minimum number of nodes and for a specific timeout.\n",
    "    # If no min_node_count is provided, it will use the scale settings for the cluster.\n",
    "    compute_target.wait_for_completion(show_output = True, min_node_count = None, timeout_in_minutes = 20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use get_status()."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize AzureML Experiment object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a name for the experiment and specify the project folder.\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Subscription ID'] = ws.subscription_id\n",
    "output['Workspace Name'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Experiment Name'] = experiment.name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Training Data\n",
    "Training data is included in the repository with the code, its read into a Pandas Dataframe to show a few sample rows. Since training occurs on a remote cluster data is uploaded to AML Workspace default datasource (Azure Blob storage) to be used by remote compute for training by the remote Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = os.path.join(os.getcwd(),'data')\n",
    "data_file = os.path.join(data_folder, 'diabetes_classification_dataset.csv')\n",
    "print(data_folder)\n",
    "print(data_file)\n",
    "\n",
    "df = pd.read_csv(data_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ws.get_default_datastore()\n",
    "ds.upload(src_dir=data_folder, target_path='diabetes_classification', overwrite=True, show_progress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Run Configuration for Aml Compute Nodes\n",
    "Training is performed on AML Compute nodes and run-time dependencies need to be specified, this includes packages needed for training as well as reference on how data will be made available to training code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr = DataReferenceConfiguration(datastore_name=ds.name, \n",
    "                   path_on_datastore='diabetes_classification', \n",
    "                   path_on_compute='/tmp/azureml_runs',\n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new RunConfig object\n",
    "conda_run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to AmlCompute\n",
    "conda_run_config.target = compute_target\n",
    "conda_run_config.environment.docker.enabled = True\n",
    "conda_run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_CPU_IMAGE\n",
    "\n",
    "# set the data reference of the run coonfiguration\n",
    "conda_run_config.data_references = {ds.name: dr}\n",
    "\n",
    "cd = CondaDependencies.create(pip_packages=['azureml-sdk[automl]'], conda_packages=['numpy','py-xgboost<=0.80'])\n",
    "conda_run_config.environment.python.conda_dependencies = cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a project_folder if it doesn't exist\n",
    "if not os.path.exists(project_folder):\n",
    "    os.makedirs(project_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./project-temp-files/get_data.py\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_data():     \n",
    "    df = pd.read_csv(\"/tmp/azureml_runs/diabetes_classification/diabetes_classification_dataset.csv\")\n",
    "    print('after pd.read_csv')    \n",
    "    # get integer labels\n",
    "    y = df[\"diabetes\"]\n",
    "    df = df.drop(\"diabetes\", axis=1)    \n",
    "    return { \"X\" : df, \"y\" : y.values }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup AutoML \n",
    "Initialize AutoML configuration and submit the training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_config = AutoMLConfig(task = 'classification',\n",
    "                             debug_log = 'automl_errors.log',\n",
    "                             path = project_folder,\n",
    "                             #compute_target = compute_target,\n",
    "                             run_configuration=conda_run_config,\n",
    "                             data_script = project_folder + \"/get_data.py\",\n",
    "                             iteration_timeout_minutes = 10,\n",
    "                             iterations = 10,\n",
    "                             n_cross_validations = 5,\n",
    "                             primary_metric = 'AUC_weighted',\n",
    "                             preprocess = True,\n",
    "                             max_concurrent_iterations = 2,\n",
    "                             verbosity= logging.INFO\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_run = experiment.submit(automl_config, show_output = False)\n",
    "remote_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(remote_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait until the run finishes.\n",
    "remote_run.wait_for_completion(show_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect and Register best Model to Model Registry\n",
    "AutoML creates mutliple models, best model is retreived for inspection and then registered with Model Registry to be used in Docker Image creation and eventually for inferencing. Models generated by AutoML can be inspected to see what transformations were applied to the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = remote_run.get_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['datatransformer'].get_engineered_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['datatransformer'].get_featurization_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = best_run.register_model(model_name = 'diabclassmodel', model_path= 'outputs/model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Docker Image to be used for Inferencing\n",
    "Create Docker Image with Scoring File, the trianed Model as well Conda dependencies to expose Web Service for inferencing.\n",
    "\n",
    "##### Azure Machine Learning and Power BI Integration\n",
    "One very important thing to note in the scoring file are the decorators <i>input_schema</i> and <i>output_schema</i> because these result in exposing a Swagger Endpoint which is used by Power BI to identify the input parameters for the service call as well as the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "# Scoring Script will need model id from registered model\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from azureml.core.model import Model\n",
    "from inference_schema.schema_decorators import input_schema, output_schema\n",
    "from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n",
    "from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n",
    "\n",
    "import azureml.train.automl\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    # retreive the path to the model file using the model name\n",
    "    model_path = Model.get_model_path('diabclassmodel') # update this based on previously registered model\n",
    "    print(model_path)\n",
    "    model = joblib.load(model_path)\n",
    "\n",
    "input_dict = {\n",
    "    \"pregnancies\": [6],\n",
    "    \"plasma glucose\": [148] ,\n",
    "    \"blood pressure\": [72],\n",
    "    \"triceps skin thickness\": [35],\n",
    "    \"insulin\": [0],\n",
    "    \"bmi\": [33.6],\n",
    "    \"diabetes pedigree\": [0.627],\n",
    "    \"age\": [50]\n",
    "}\n",
    "\n",
    "output_dict = {\n",
    "    \"prediction\": [1],\n",
    "    \"probability\": [.89]\n",
    "}\n",
    "\n",
    "input_sample = pd.DataFrame(input_dict)\n",
    "output_sample =  pd.DataFrame(output_dict)\n",
    "#output_sample = np.array([(\"1\",.90), (\"0\",.84)])\n",
    "\n",
    "@input_schema('data', PandasParameterType(input_sample))\n",
    "@output_schema(PandasParameterType(output_sample))\n",
    "def run(data):\n",
    "    # grab and prepare the data\n",
    "    # make prediction\n",
    "    try:\n",
    "        print('inside the method')                      \n",
    "        result_df = pd.DataFrame(columns = [\"prediction\",\"probability\"]) \n",
    "        \n",
    "        pred = model.predict(data)\n",
    "        prob = model.predict_proba(data)\n",
    "        \n",
    "        print(pred)\n",
    "        print(prob)\n",
    "                \n",
    "        for idx,val in enumerate(pred):\n",
    "            print(\"index:\",idx, \"value:\", val)\n",
    "            print(val)\n",
    "            print(prob[idx][int(val)])\n",
    "            result_df = result_df.append({\"prediction\": val, \"probability\": prob[idx][int(val)]}, ignore_index=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Exception Caught\")\n",
    "        print(str(e))\n",
    "        return [\"exception\", str(e)]    \n",
    "    str = result_df.to_json(orient = 'records')    \n",
    "    return json.loads(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myenv = CondaDependencies.create(conda_packages=['numpy','scikit-learn'], pip_packages=['azureml-sdk[automl]', 'inference-schema[numpy-support,pandas-support]'])\n",
    "\n",
    "conda_env_file_name = 'mydeployenv.yml'\n",
    "myenv.save_to_file('.', conda_env_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.image import Image, ContainerImage\n",
    "\n",
    "image_config = ContainerImage.image_configuration(runtime= \"python\",\n",
    "                                 execution_script=\"score.py\",\n",
    "                                 conda_file=\"mydeployenv.yml\",\n",
    "                                 tags = {'area': \"diabetes\", 'type': \"classification\"},\n",
    "                                 description = \"Diabetes Classification with probability implemented using AutoML\")\n",
    "\n",
    "image = Image.create(name = image_name,\n",
    "                     # this is the model object. note you can pass in 0-n models via this list-type parameter\n",
    "                     # in case you need to reference multiple models, or none at all, in your scoring script.\n",
    "                     models = [model],\n",
    "                     image_config = image_config, \n",
    "                     workspace = ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.wait_for_creation(show_output = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
